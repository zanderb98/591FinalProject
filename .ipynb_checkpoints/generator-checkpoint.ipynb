{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "NOISE_DIM = 38804"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(noise_dim=NOISE_DIM):\n",
    "    \"\"\"\n",
    "    Build and return a PyTorch model implementing the architecture above.\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(\n",
    "         nn.Linear(noise_dim, 1024),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(1024, 1024),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(1024, 784),\n",
    "         nn.Tanh()\n",
    "    )\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_noise(batch_size, dim):\n",
    "    \"\"\"\n",
    "    Generate a PyTorch Tensor of uniform random noise.\n",
    "\n",
    "    Input:\n",
    "    - batch_size: Integer giving the batch size of noise to generate.\n",
    "    - dim: Integer giving the dimension of noise to generate.\n",
    "\n",
    "    Output:\n",
    "    - A PyTorch Tensor of shape (batch_size, dim) containing uniform\n",
    "      random noise in the range (-1, 1).\n",
    "    \"\"\"\n",
    "    return (torch.rand(batch_size, dim) * 2) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "class Unflatten(nn.Module):\n",
    "    \"\"\"\n",
    "    An Unflatten module receives an input of shape (N, CHW) and reshapes it\n",
    "    to produce an output of shape (N, C, H, W).\n",
    "    \"\"\"\n",
    "    def init(self, N=-1, C=128, H=7, W=7):\n",
    "        super(Unflatten, self).init()\n",
    "        self.N = N\n",
    "        self.C = C\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "    def forward(self, x):\n",
    "        return x.view(self.N, self.C, self.H, self.W)\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavieruniform(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model):\n",
    "    \"\"\"\n",
    "    Construct and return an Adam optimizer for the model with learning rate 1e-3,\n",
    "    beta1=0.5, and beta2=0.999.\n",
    "\n",
    "    Input:\n",
    "    - model: A PyTorch model that we want to optimize.\n",
    "\n",
    "    Returns:\n",
    "    - An Adam optimizer for the model with the desired hyperparameters.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dc_generator(noise_dim=NOISE_DIM):\n",
    "    \"\"\"\n",
    "    Build and return a PyTorch model implementing the DCGAN generator using\n",
    "    the architecture described above.\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(noise_dim, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.Linear(1024, 6272),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(6272),\n",
    "        Unflatten(batch_size, 128, 7, 7),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
    "        nn.Tanh(),\n",
    "        Flatten()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(logits_fake):\n",
    "    \"\"\"\n",
    "    Computes the generator loss described above.\n",
    "\n",
    "    Inputs:\n",
    "    - logits_fake: PyTorch Tensor of shape (N,) giving scores for the fake data.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Tensor containing the (scalar) loss for the generator.\n",
    "    \"\"\"\n",
    "    loss = bce_loss(logits_fake, torch.ones(logits_fake.size()[0]).type(dtype))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_generator_loss(scores_fake):\n",
    "    \"\"\"\n",
    "    Computes the Least-Squares GAN loss for the generator.\n",
    "    \n",
    "    Inputs:\n",
    "    - scores_fake: PyTorch Tensor of shape (N,) giving scores for the fake data.\n",
    "    \n",
    "    Outputs:\n",
    "    - loss: A PyTorch Tensor containing the loss.\n",
    "    \"\"\"\n",
    "    loss = torch.mean(torch.pow(scores_fake - torch.ones(scores_fake.size()[0]).type(dtype), 2)) / 2\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
